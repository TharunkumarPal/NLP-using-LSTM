{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras import optimizers\n",
    "tf.keras.layers.Embedding\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cleaned1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Downloads\\RNN_NLP.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Downloads/RNN_NLP.ipynb#ch0000001?line=0'>1</a>\u001b[0m data\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mcleaned1.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Downloads/RNN_NLP.ipynb#ch0000001?line=1'>2</a>\u001b[0m data\u001b[39m.\u001b[39mhead(\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned1.csv'"
     ]
    }
   ],
   "source": [
    "data= pd.read_csv('cleaned1.csv')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=data.drop(columns='Unnamed: 0',inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= data['review_body'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review_body']=df.apply(lambda x: re.sub('[^a-zA-Z ]', '', x))\n",
    "data['review_body']=df.apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provides great cushion as well as archsupport</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's perfect if you need something small for c...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>getting what u see</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my 13 year old son loved these shoes excellent...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>shoes began to &amp;#34;peel&amp;#34; within a month. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>i had ordered this bag in the herbal mist colo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>great shoe but after wearing them for less the...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>it's a pretty shoe but runs small</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>they squeak all the time. do not like them.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review_body  star_rating  \\\n",
       "0          provides great cushion as well as archsupport            5   \n",
       "1      it's perfect if you need something small for c...            4   \n",
       "2                                     getting what u see            5   \n",
       "3                                                  small            3   \n",
       "4      my 13 year old son loved these shoes excellent...            5   \n",
       "...                                                  ...          ...   \n",
       "99995  shoes began to &#34;peel&#34; within a month. ...            1   \n",
       "99996  i had ordered this bag in the herbal mist colo...            1   \n",
       "99997  great shoe but after wearing them for less the...            2   \n",
       "99998                  it's a pretty shoe but runs small            2   \n",
       "99999        they squeak all the time. do not like them.            1   \n",
       "\n",
       "       sentiment  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "...          ...  \n",
       "99995          0  \n",
       "99996          0  \n",
       "99997          0  \n",
       "99998          0  \n",
       "99999          0  \n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_reviews(review):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens= review_text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    #le=WordNetLemmatizer()\n",
    "    #stop_words= set(stopwords.words(\"english\"))     \n",
    "    #word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review=\" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK LIBRARY FOR CREATING TOKENIZING REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280197\n",
      "280197\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences=[]\n",
    "sum=0\n",
    "for review_body in data['review_body']:\n",
    "    sents=tokenizer.tokenize(review_body.strip())\n",
    "    sum+=len(sents)\n",
    "    for sent in sents:\n",
    "        cleaned_sent=clean_reviews(sent)\n",
    "        sentences.append(cleaned_sent.split()) # can use word_tokenize also.\n",
    "print(sum)\n",
    "print(len(sentences))  # total no of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_model=gensim.models.Word2Vec(sentences=sentences, vector_size=300, window=10,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23800398, 33875000)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01482964,  0.05365279,  0.21364582,  1.509128  ,  0.19120501,\n",
       "       -0.45705885,  0.69639635,  0.12131977, -0.1970114 , -1.5681069 ,\n",
       "       -1.3750471 , -0.09908123, -0.18555643,  1.078124  , -1.8970355 ,\n",
       "       -0.37271693,  0.3267752 ,  2.32171   , -0.04179703, -1.3103684 ,\n",
       "        0.2839957 , -0.80827445, -0.6977456 , -0.24594198, -0.01212678,\n",
       "       -0.68564117,  0.6389927 , -0.4106011 ,  0.0801646 , -0.65511036,\n",
       "       -0.19347282,  1.1372072 , -1.1358166 , -0.9680011 ,  1.9426973 ,\n",
       "        2.0871346 , -0.6104485 ,  0.7143133 , -1.5073532 ,  1.4545008 ,\n",
       "        1.9574941 ,  0.00508789, -1.7841923 , -0.94573987,  0.32479912,\n",
       "        0.0405994 ,  1.5777903 ,  1.2236146 ,  0.23646735,  1.3863181 ,\n",
       "        0.546704  ,  1.1135038 , -0.50789195,  0.57979393, -0.15880561,\n",
       "       -0.6522637 ,  0.8495707 , -0.48905239,  1.0652207 , -0.3079095 ,\n",
       "        0.30141208,  0.02429047,  1.0293454 ,  1.1065664 , -0.13563903,\n",
       "       -0.11545537,  0.37891278,  0.7820947 ,  0.35851318, -1.1973083 ,\n",
       "       -0.8171671 , -0.5982907 , -1.0006915 , -1.1679504 ,  0.08800227,\n",
       "        1.2200966 ,  0.48176992, -0.7254642 , -1.573041  ,  0.16343647,\n",
       "       -1.5573593 ,  0.05500713, -1.3157747 , -1.4057369 ,  0.54189855,\n",
       "        0.83586836,  0.8116176 , -1.7022635 ,  0.6728359 , -0.2900906 ,\n",
       "        0.414319  , -1.8787583 ,  0.27751112,  2.7081416 ,  0.30134836,\n",
       "       -0.08508467, -1.0230551 , -0.063399  ,  0.9107453 ,  0.17383479,\n",
       "        0.80299044, -1.1870524 ,  1.2536672 , -1.0074986 , -0.5209932 ,\n",
       "        1.0063635 ,  1.0084571 , -2.4848042 , -1.153335  , -0.18981978,\n",
       "        0.23987824,  1.0066495 , -0.7171699 ,  0.2830543 , -0.40849388,\n",
       "        1.1788597 , -0.31925043,  0.1552451 ,  1.789292  , -1.2259762 ,\n",
       "        0.9176199 ,  0.53447324, -0.71499836, -1.6712312 ,  0.07381285,\n",
       "        1.1848037 , -0.6151312 ,  0.7088632 ,  2.453966  ,  0.9038138 ,\n",
       "        0.4849096 ,  1.2751424 , -1.1194075 ,  0.95699257, -0.3602652 ,\n",
       "       -0.32840073, -0.29659593,  0.81102884,  1.1903017 , -0.64551646,\n",
       "        0.0084799 ,  0.36611608, -0.8704629 , -0.16391852, -0.6941439 ,\n",
       "       -0.8934302 ,  1.071658  , -0.9523582 , -1.1465021 , -0.7920531 ,\n",
       "       -0.2831683 ,  1.1396334 ,  0.71224135, -1.7341282 ,  0.58635324,\n",
       "        0.27082235,  2.1563804 ,  1.9112844 ,  0.43963948,  0.2929266 ,\n",
       "        1.2247192 , -0.377326  ,  0.5869946 ,  0.649644  ,  0.970973  ,\n",
       "       -1.0123737 , -0.8235108 , -0.5502232 ,  1.9900262 , -1.2074232 ,\n",
       "       -0.5095004 , -0.6789042 , -0.59351206, -1.1927097 , -0.5294192 ,\n",
       "        1.4697711 , -0.7672691 ,  0.8077091 ,  0.66935474, -0.00397059,\n",
       "        0.21203811,  1.6506896 , -0.91380876,  1.6662358 , -0.6556518 ,\n",
       "       -0.99560326,  0.6222701 , -0.41404864, -0.6827292 ,  0.6251237 ,\n",
       "        0.7724504 , -0.6686064 ,  1.1022761 , -0.6253335 , -0.21727704,\n",
       "        1.6681801 , -1.5910157 , -1.9351982 ,  0.2933014 , -0.7058039 ,\n",
       "        0.67607355,  0.2929357 , -0.5709993 ,  0.18796332,  0.4532488 ,\n",
       "       -0.8146027 , -1.0935181 ,  0.9823182 ,  1.590539  , -1.5293684 ,\n",
       "       -0.42101806,  0.22382873,  1.222749  , -0.27332434, -1.5573099 ,\n",
       "       -1.0682122 , -1.1742569 , -0.07931805, -0.27283144,  0.06564365,\n",
       "       -0.04343195, -0.1300192 ,  1.6477178 ,  0.69964254,  1.8108274 ,\n",
       "        1.1729839 ,  1.038916  , -0.39462563, -0.5951057 , -0.28999457,\n",
       "       -2.069478  ,  0.45768875, -1.3621594 ,  1.4348621 , -1.172845  ,\n",
       "       -0.8697572 ,  0.26434445, -0.45877275, -0.48430946,  0.10857145,\n",
       "        1.6871084 ,  0.22893466, -0.25436535,  0.4604691 , -0.3637804 ,\n",
       "       -0.6942748 , -1.0797012 , -2.0411103 ,  0.97178364,  0.12486455,\n",
       "        1.2984227 , -0.36549535, -0.60969925, -0.10856759, -0.03719896,\n",
       "        0.07116282,  0.47379142,  1.3546193 ,  1.1788719 , -2.5404763 ,\n",
       "       -0.50477093, -1.0876982 , -0.48631206, -2.7157784 , -0.76209396,\n",
       "        0.30115777, -1.269443  , -0.3543374 , -1.9859343 , -0.623856  ,\n",
       "        0.02156595,  0.646958  ,  0.63747644,  0.45776567, -0.5954347 ,\n",
       "       -1.2093147 , -0.5181677 , -0.41691574,  2.299336  ,  0.78812116,\n",
       "       -0.19651301,  0.5745008 ,  2.2947874 ,  0.8281105 , -0.35786954,\n",
       "        1.2389536 ,  0.40223753,  0.0079311 , -1.1790512 , -0.88827634,\n",
       "        0.08816334, -0.52791804, -0.11447226,  0.0549196 , -0.949443  ,\n",
       "       -0.06594181, -0.65278804, -0.78832614, -1.2292297 ,  1.0740725 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding of a particular word.\n",
    "w2v_model.wv.get_vector('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words are :  29402\n"
     ]
    }
   ],
   "source": [
    "# total numberof extracted words.\n",
    "vocab_len=len(w2v_model.wv)\n",
    "print(\"The total number of words are : \", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab)\n",
    "vocab=w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 0.7815357446670532),\n",
       " ('fantastic', 0.760620653629303),\n",
       " ('wonderful', 0.7012671232223511),\n",
       " ('awesome', 0.6933570504188538),\n",
       " ('amazing', 0.6709727644920349),\n",
       " ('nice', 0.6701613664627075),\n",
       " ('excellent', 0.6624512672424316),\n",
       " ('decent', 0.626084566116333),\n",
       " ('terrific', 0.6160770058631897),\n",
       " ('terrible', 0.6078154444694519)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most similar to a given word.\n",
    "w2v_model.wv.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7815358"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similaraity b/w two words\n",
    "w2v_model.wv.similarity('good','great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning reviews.\n",
    "df['clean_review']=data['review_body'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1720\n"
     ]
    }
   ],
   "source": [
    "# number of unique words = 56379.\n",
    "\n",
    "# now since we will have to pad we need to find the maximum lenght of any document.\n",
    "\n",
    "maxi=-1\n",
    "for i,rev in enumerate(df['clean_review']):\n",
    "    tokens=rev.split()\n",
    "    if(len(tokens)>maxi):\n",
    "        maxi=len(tokens)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(df['clean_review'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encd_rev = tok.texts_to_sequences(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rev_len=1720  # max lenght of a review\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim=300 # embedding dimension as choosen in word2vec constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1720)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now padding to have a amximum length of 1565\n",
    "pad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
    "pad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.word_index.items():\n",
    "    embed_vector=vocab.get_vector(word)\n",
    "    if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "        embed_matrix[i]=embed_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.99682865e-02 -1.05259621e+00 -6.29021287e-01 -5.59548795e-01\n",
      "  1.19127190e+00  8.99894059e-01  5.09357274e-01 -7.48626471e-01\n",
      " -4.90565687e-01 -7.85346925e-01 -3.14486057e-01 -4.83407408e-01\n",
      " -1.14927195e-01  4.14361179e-01  9.12559748e-01  4.57611047e-02\n",
      " -4.11298841e-01 -1.64394641e+00 -4.66870606e-01  5.99127233e-01\n",
      " -2.94571612e-02  7.45729148e-01 -1.40267849e+00 -5.59033990e-01\n",
      " -9.69717383e-01  6.62935793e-01 -2.89381051e+00 -3.99250239e-02\n",
      " -7.27586687e-01 -6.72767997e-01  5.28688729e-01 -9.94669348e-02\n",
      "  1.29635796e-01 -1.05314624e+00 -1.13817680e+00 -1.31791365e+00\n",
      " -1.64992487e+00 -1.16549827e-01  4.28857595e-01  1.22088015e+00\n",
      " -9.73775923e-01  8.10942471e-01  1.16047315e-01  2.17652410e-01\n",
      " -1.34454262e+00 -1.07243143e-01 -8.78000259e-01 -2.34663531e-01\n",
      "  1.00955176e+00  3.18399698e-01  5.30542195e-01 -1.79542625e+00\n",
      "  6.23048663e-01 -6.01855636e-01  5.40060282e-01  1.46226808e-01\n",
      "  5.37211001e-01  2.14100733e-01 -4.95496958e-01  3.91541213e-01\n",
      " -2.71837227e-02 -1.57672179e+00  7.51404226e-01 -5.31928778e-01\n",
      " -5.18374860e-01 -2.83029765e-01 -1.54433799e+00 -2.04688787e-01\n",
      "  2.14812875e+00 -1.00472116e+00 -5.13786256e-01 -1.56641722e-01\n",
      "  3.60013634e-01 -9.96572673e-01  4.28325385e-01 -5.21073453e-02\n",
      "  3.50008965e-01 -7.75338352e-01 -6.80386364e-01  5.20299412e-02\n",
      "  2.41538748e-01  5.96830964e-01  5.85997820e-01 -1.88065231e+00\n",
      "  2.65386462e+00 -3.95120770e-01  3.32784593e-01 -6.69817746e-01\n",
      " -1.90361059e+00 -6.42682752e-03 -2.42817730e-01  8.30191553e-01\n",
      "  1.07367563e+00  2.64549404e-01  2.16771185e-01 -3.15334290e-01\n",
      " -1.80848673e-01 -1.58029962e-02  7.49612272e-01 -1.25683737e+00\n",
      " -4.39644545e-01 -2.38766283e-01 -1.32620919e+00 -7.64652133e-01\n",
      " -2.82618451e+00  3.58432829e-01  2.09550381e+00 -1.82898697e-02\n",
      " -3.76044363e-01  5.97155571e-01  1.63445139e+00  2.22297441e-02\n",
      "  1.66950762e-01  6.98953867e-01 -9.20035243e-01 -2.26953626e-01\n",
      "  1.65702546e+00  6.75050840e-02  8.65427077e-01  3.24353576e-01\n",
      " -8.75996828e-01 -5.54680228e-01  1.71998337e-01  5.17980874e-01\n",
      "  3.44299711e-02  2.87541514e-03 -1.20455727e-01 -5.95552385e-01\n",
      "  1.29824027e-01  6.26750469e-01  1.38487756e-01  6.29156947e-01\n",
      "  1.38801217e+00 -9.31032419e-01 -6.01589978e-01 -1.42593995e-01\n",
      " -3.17975789e-01  2.18934584e+00  5.83798230e-01 -8.23239237e-02\n",
      "  4.85403389e-01  1.14562237e+00  2.18962491e-01  2.05102757e-01\n",
      "  1.93610811e+00  1.34668517e+00 -1.16502535e+00  8.14264774e-01\n",
      "  3.09830189e-01  1.13817215e+00 -1.50866544e+00  1.11126518e+00\n",
      " -1.36912978e+00 -4.38520044e-01  4.83933657e-01 -5.82564414e-01\n",
      " -1.02719200e+00  4.83868182e-01 -5.36419928e-01  9.78571653e-01\n",
      " -5.06198764e-01 -1.72822428e+00  4.22939122e-01  8.58284295e-01\n",
      "  1.20454919e+00 -1.95331693e-01 -1.24391890e+00  9.75960314e-01\n",
      "  5.16965091e-01 -2.23988175e-01 -3.17568965e-02  1.50695398e-01\n",
      "  1.54530132e+00 -3.28989893e-01 -6.70673370e-01 -3.61348867e-01\n",
      "  6.65990949e-01 -1.23609233e+00 -8.46519172e-01  1.11195421e+00\n",
      " -7.49869645e-02 -5.68302095e-01 -1.61054683e+00  1.37248695e+00\n",
      " -2.06460571e+00 -6.02346063e-01  1.27456218e-01 -4.05458272e-01\n",
      " -5.53306282e-01 -6.50260627e-01  6.11413360e-01  5.79663634e-01\n",
      " -1.41015422e+00  8.99001360e-01  1.72270298e+00  1.05578780e+00\n",
      "  3.03596765e-01 -1.39643526e+00 -6.90769970e-01  2.27248383e+00\n",
      " -7.15460002e-01 -8.56779993e-01 -7.89054036e-01 -3.40111554e-01\n",
      "  1.91879630e+00  2.59743392e-01  1.50072351e-01 -1.14356779e-01\n",
      "  1.57495245e-01 -9.11327779e-01  1.94983765e-01  2.96993583e-01\n",
      " -4.28224534e-01  1.57615626e+00 -4.97961313e-01  8.17343652e-01\n",
      "  1.90252924e+00 -8.34762394e-01 -1.66004926e-01 -1.98814464e+00\n",
      "  2.25578487e-01  1.56305447e-01 -6.11474335e-01 -1.20320320e-01\n",
      " -6.85677707e-01  8.65610465e-02 -3.73935848e-02  5.28679907e-01\n",
      " -9.86607909e-01 -1.04962893e-01 -2.67703819e+00  1.32903445e+00\n",
      " -3.83761346e-01 -3.08897406e-01 -5.47905445e-01  8.94615948e-01\n",
      "  1.33129728e+00 -3.54841709e-01  3.70042622e-01  7.10928813e-02\n",
      " -1.13322258e+00  2.26565853e-01  1.40972817e+00 -1.51310906e-01\n",
      "  1.60971594e+00  1.16600144e+00 -1.57356226e+00 -9.79379833e-01\n",
      " -4.30224016e-02 -1.46227682e+00  4.36081052e-01 -9.48302388e-01\n",
      " -1.85338020e+00  1.07373083e+00 -4.71953839e-01  5.77746272e-01\n",
      " -1.39158284e-02 -4.15423125e-01  5.83508551e-01 -2.49318600e-01\n",
      " -2.14692020e+00 -1.83469579e-01  2.18296719e+00  7.14281499e-01\n",
      " -3.50176960e-01 -1.31915426e+00 -1.22126877e+00 -1.95184812e-01\n",
      "  5.38780615e-02 -3.81655008e-01  6.95986033e-04  9.47562993e-01\n",
      " -7.89608717e-01  6.15799904e-01 -1.16552138e+00 -3.11247617e-01\n",
      "  2.12899017e+00 -9.70808983e-01 -8.50185037e-01  9.26768243e-01\n",
      " -7.57591724e-01  2.09103480e-01 -5.03232956e-01 -4.28945422e-01\n",
      "  1.03428261e-03  1.57179201e+00  1.77836657e+00  9.53911543e-01\n",
      " -2.19467476e-01 -4.11000326e-02 -6.23139858e-01  4.82344061e-01\n",
      "  2.34107137e-01  1.56249836e-01 -1.34541154e+00 -7.70949483e-01\n",
      "  9.86492038e-01  1.82117194e-01 -2.25359440e+00  3.12550831e+00]\n"
     ]
    }
   ],
   "source": [
    "# checking.\n",
    "print(embed_matrix[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and val sets first\n",
    "#Y=keras.utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\n",
    "x_train,x_test,y_train,y_test=train_test_split(pad_rev,data['sentiment'],test_size=0.10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "#y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))\n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test = np.asarray(y_test).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM MODEL GENERATION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 128\n",
    "dropout = 0.2\n",
    "epochs = 3\n",
    "batch_size = 128\n",
    "maxlen=1720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1720, 128)         35865216  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,996,929\n",
      "Trainable params: 35,996,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(tf.keras.layers.LSTM(layers))\n",
    "model.add(tf.keras.layers.Dropout(dropout))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc',color=\"red\")\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss',color=\"red\")\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b302cdd1e032ee910f5c889c3360c28564c92ad4f326fc3102e39fbe47faee66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
